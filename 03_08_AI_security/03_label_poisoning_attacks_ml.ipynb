{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "822ac6d0",
   "metadata": {},
   "source": [
    "\n",
    "Data Poisoning attacks on Neural Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cac0b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b13947c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1b5c237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA POISONING ATTACKS\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DATA POISONING ATTACKS\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1338993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3dafa1c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f7ced73",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define a simple CNN architecture\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b53741b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, train_loader, epochs=3, device='cpu'):\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "327913fd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader, device='cpu'):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            total += target.size(0)\n",
    "            all_predictions.extend(pred.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy, all_predictions, all_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b32b2978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf2c026c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "787c970a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training baseline model...\n",
      "Epoch [1/10], Loss: 0.2057\n",
      "Epoch [2/10], Loss: 0.0506\n",
      "Epoch [3/10], Loss: 0.0318\n",
      "Epoch [4/10], Loss: 0.0231\n",
      "Epoch [5/10], Loss: 0.0168\n",
      "Epoch [6/10], Loss: 0.0124\n",
      "Epoch [7/10], Loss: 0.0113\n",
      "Epoch [8/10], Loss: 0.0073\n",
      "Epoch [9/10], Loss: 0.0070\n",
      "Epoch [10/10], Loss: 0.0058\n"
     ]
    }
   ],
   "source": [
    "# Train baseline model\n",
    "print(\"Training baseline model...\")\n",
    "baseline_model = SimpleCNN().to(device)\n",
    "train_model(baseline_model, train_loader, epochs=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efcfd6be",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline model evaluation:\n",
      "Test Accuracy: 98.87%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate baseline model\n",
    "print(\"\\nBaseline model evaluation:\")\n",
    "baseline_acc, baseline_preds, baseline_targets = evaluate_model(baseline_model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69353762",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Label Flipping Attack\n",
    "class LabelFlippingAttack:\n",
    "    def __init__(self, poison_ratio=0.2, target_class=3):\n",
    "        self.poison_ratio = poison_ratio\n",
    "        self.target_class = target_class\n",
    "    \n",
    "    def poison_dataset(self, dataset):\n",
    "        \"\"\"Flip labels of a portion of the dataset to a target class\"\"\"\n",
    "        poisoned_dataset = deepcopy(dataset)\n",
    "        total_samples = len(poisoned_dataset)\n",
    "        num_poisoned = int(total_samples * self.poison_ratio)\n",
    "        \n",
    "        # Select random indices to poison\n",
    "        indices_to_poison = random.sample(range(total_samples), num_poisoned)\n",
    "        \n",
    "        for idx in indices_to_poison:\n",
    "            data, _ = poisoned_dataset[idx]\n",
    "            # Flip the label to target class\n",
    "            poisoned_dataset.targets[idx] = self.target_class\n",
    "        \n",
    "        print(f\"Poisoned {num_poisoned} samples with random label flipping attack\")\n",
    "        return poisoned_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d439f52",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Optimized Label Flipping Attack\n",
    "class OptimizedLabelFlippingAttack:\n",
    "    def __init__(self, poison_ratio=0.2, target_class=3, selection_method='gradient'):\n",
    "        self.poison_ratio = poison_ratio\n",
    "        self.target_class = target_class\n",
    "        self.selection_method = selection_method  # 'gradient', 'confidence', 'loss'\n",
    "    \n",
    "    def compute_gradient_importance(self, model, dataset, num_samples=10000):\n",
    "        \"\"\"Compute gradient-based importance scores for label flipping selection\"\"\"\n",
    "        model.eval()\n",
    "        importance_scores = []\n",
    "        \n",
    "        # Sample a subset for efficiency\n",
    "        indices = random.sample(range(len(dataset)), min(num_samples, len(dataset)))\n",
    "        \n",
    "        for idx in indices:\n",
    "            data, target = dataset[idx]\n",
    "            data = data.unsqueeze(0).to(device)\n",
    "            target = torch.tensor([target]).to(device)\n",
    "            \n",
    "            # Compute gradients with respect to input\n",
    "            data.requires_grad_(True)\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Compute gradient magnitude as importance score\n",
    "            grad_magnitude = torch.norm(data.grad).item()\n",
    "            importance_scores.append((idx, grad_magnitude))\n",
    "            \n",
    "            data.requires_grad_(False)\n",
    "        \n",
    "        # Sort by importance (highest gradient magnitude first)\n",
    "        importance_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        return importance_scores\n",
    "    \n",
    "    def compute_confidence_importance(self, model, dataset, num_samples=10000):\n",
    "        \"\"\"Compute confidence-based importance scores for label flipping selection\"\"\"\n",
    "        model.eval()\n",
    "        importance_scores = []\n",
    "        \n",
    "        # Sample a subset for efficiency\n",
    "        indices = random.sample(range(len(dataset)), min(num_samples, len(dataset)))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for idx in indices:\n",
    "                data, target = dataset[idx]\n",
    "                data = data.unsqueeze(0).to(device)\n",
    "                \n",
    "                # Get model prediction and confidence\n",
    "                output = model(data)\n",
    "                probabilities = F.softmax(output, dim=1)\n",
    "                confidence = probabilities.max().item()\n",
    "                predicted_class = output.argmax().item()\n",
    "                \n",
    "                # Higher importance for samples that are confidently predicted correctly\n",
    "                # These will have the most impact when flipped\n",
    "                if predicted_class == target:\n",
    "                    importance_score = confidence  # High confidence correct predictions\n",
    "                else:\n",
    "                    importance_score = 1 - confidence  # Low confidence incorrect predictions\n",
    "                \n",
    "                importance_scores.append((idx, importance_score))\n",
    "        \n",
    "        # Sort by importance (highest confidence first)\n",
    "        importance_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        return importance_scores\n",
    "    \n",
    "    def compute_loss_importance(self, model, dataset, num_samples=10000):\n",
    "        \"\"\"Compute loss-based importance scores for label flipping selection\"\"\"\n",
    "        model.eval()\n",
    "        importance_scores = []\n",
    "        \n",
    "        # Sample a subset for efficiency\n",
    "        indices = random.sample(range(len(dataset)), min(num_samples, len(dataset)))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for idx in indices:\n",
    "                data, target = dataset[idx]\n",
    "                data = data.unsqueeze(0).to(device)\n",
    "                target = torch.tensor([target]).to(device)\n",
    "                \n",
    "                # Compute loss\n",
    "                output = model(data)\n",
    "                loss = F.cross_entropy(output, target).item()\n",
    "                \n",
    "                # Lower loss means higher importance (more confident predictions)\n",
    "                importance_score = 1.0 / (1.0 + loss)  # Transform to [0, 1] range\n",
    "                importance_scores.append((idx, importance_score))\n",
    "        \n",
    "        # Sort by importance (highest first)\n",
    "        importance_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        return importance_scores\n",
    "    \n",
    "    def poison_dataset(self, dataset, model):\n",
    "        \"\"\"Poison dataset using optimized label flipping selection\"\"\"\n",
    "        poisoned_dataset = deepcopy(dataset)\n",
    "        total_samples = len(poisoned_dataset)\n",
    "        num_poisoned = int(total_samples * self.poison_ratio)\n",
    "        \n",
    "        print(f\"Computing {self.selection_method}-based importance scores...\")\n",
    "        \n",
    "        if self.selection_method == 'gradient':\n",
    "            importance_scores = self.compute_gradient_importance(model, dataset)\n",
    "        elif self.selection_method == 'confidence':\n",
    "            importance_scores = self.compute_confidence_importance(model, dataset)\n",
    "        elif self.selection_method == 'loss':\n",
    "            importance_scores = self.compute_loss_importance(model, dataset)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown selection method: {self.selection_method}\")\n",
    "        \n",
    "        # Select top-k most important samples for poisoning\n",
    "        selected_indices = [idx for idx, _ in importance_scores[:num_poisoned]]\n",
    "        \n",
    "        print(f\"Selected {len(selected_indices)} samples based on {self.selection_method} importance\")\n",
    "        \n",
    "        # Flip labels of selected samples\n",
    "        for idx in selected_indices:\n",
    "            poisoned_dataset.targets[idx] = self.target_class\n",
    "        \n",
    "        print(f\"Poisoned {len(selected_indices)} samples with optimized label flipping attack\")\n",
    "        return poisoned_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915bcb50",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Backgradient Poisoning Attack\n",
    "class BackgradientPoisoningAttack:\n",
    "    def __init__(self, poison_ratio=0.2, target_class=3, optimization_steps=100, lr=0.01):\n",
    "        self.poison_ratio = poison_ratio\n",
    "        self.target_class = target_class\n",
    "        self.optimization_steps = optimization_steps\n",
    "        self.lr = lr\n",
    "    \n",
    "    def compute_gradient_importance(self, model, dataset, num_samples=10000):\n",
    "        \"\"\"Compute gradient-based importance scores for data selection\"\"\"\n",
    "        model.eval()\n",
    "        importance_scores = []\n",
    "        \n",
    "        # Sample a subset for efficiency\n",
    "        indices = random.sample(range(len(dataset)), min(num_samples, len(dataset)))\n",
    "        \n",
    "        for idx in indices:\n",
    "            data, target = dataset[idx]\n",
    "            data = data.unsqueeze(0).to(device)\n",
    "            target = torch.tensor([target]).to(device)\n",
    "            \n",
    "            # Compute gradients with respect to input\n",
    "            data.requires_grad_(True)\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Compute gradient magnitude as importance score\n",
    "            grad_magnitude = torch.norm(data.grad).item()\n",
    "            importance_scores.append((idx, grad_magnitude))\n",
    "            \n",
    "            data.requires_grad_(False)\n",
    "        \n",
    "        # Sort by importance (highest gradient magnitude first)\n",
    "        importance_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        return importance_scores\n",
    "    \n",
    "    def poison_dataset(self, dataset, model):\n",
    "        \"\"\"Poison dataset using gradient-based optimization\"\"\"\n",
    "        poisoned_dataset = deepcopy(dataset)\n",
    "        total_samples = len(poisoned_dataset)\n",
    "        num_poisoned = int(total_samples * self.poison_ratio)\n",
    "        \n",
    "        print(f\"Computing gradient-based importance scores...\")\n",
    "        importance_scores = self.compute_gradient_importance(model, dataset)\n",
    "        \n",
    "        # Select top-k most important samples for poisoning\n",
    "        selected_indices = [idx for idx, _ in importance_scores[:num_poisoned]]\n",
    "        \n",
    "        print(f\"Selected {len(selected_indices)} samples based on gradient importance\")\n",
    "        \n",
    "        # Poison selected samples with optimized perturbations\n",
    "        for idx in selected_indices:\n",
    "            data, _ = poisoned_dataset[idx]\n",
    "            data = data.unsqueeze(0).to(device)\n",
    "            \n",
    "            # Optimize perturbation to maximize loss\n",
    "            perturbation = torch.randn_like(data) * 0.1\n",
    "            perturbation.requires_grad_(True)\n",
    "            \n",
    "            optimizer = optim.Adam([perturbation], lr=self.lr)\n",
    "            \n",
    "            for step in range(self.optimization_steps):\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Add perturbation to data\n",
    "                perturbed_data = torch.clamp(data + perturbation, 0, 1)\n",
    "                \n",
    "                # Compute output\n",
    "                output = model(perturbed_data)\n",
    "                \n",
    "                # Create adversarial example that will be misclassified\n",
    "                # We want to maximize the loss for the original class\n",
    "                original_target = torch.tensor([poisoned_dataset.targets[idx]]).to(device)\n",
    "                \n",
    "                # Method 1: Maximize loss for original class\n",
    "                adversarial_loss = -F.cross_entropy(output, original_target)\n",
    "                \n",
    "                # Method 2: Minimize confidence in correct prediction\n",
    "                probs = F.softmax(output, dim=1)\n",
    "                correct_prob = probs[0, poisoned_dataset.targets[idx]]\n",
    "                confidence_loss = correct_prob\n",
    "                \n",
    "                # Method 3: Maximize confidence in wrong prediction (poisoning target)\n",
    "                # Find the class with second highest probability (not the original)\n",
    "                probs_sorted, indices = torch.sort(probs, descending=True)\n",
    "                wrong_class = indices[0, 1] if indices[0, 0] == poisoned_dataset.targets[idx] else indices[0, 0]\n",
    "                wrong_confidence_loss = -probs[0, wrong_class]  # Maximize confidence in wrong class\n",
    "                \n",
    "                # Combined loss - be more aggressive\n",
    "                loss = adversarial_loss + confidence_loss + 0.5 * wrong_confidence_loss\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Project perturbation to valid range\n",
    "                #with torch.no_grad(): # don't track the gradients\n",
    "                #    perturbation.clamp_(-0.5, 0.5)  # Allow larger perturbations\n",
    "            \n",
    "            # Apply optimized perturbation\n",
    "            final_perturbed_data = torch.clamp(data + perturbation, 0, 1)\n",
    "            poisoned_dataset.data[idx] = (final_perturbed_data.squeeze() * 255).to(torch.uint8)\n",
    "            poisoned_dataset.targets[idx] = self.target_class\n",
    "        \n",
    "        print(f\"Poisoned {len(selected_indices)} samples with backgradient poisoning\")\n",
    "        return poisoned_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1dad1467",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Witches' Brew Poisoning Attack\n",
    "class WitchesBrewPoisoningAttack:\n",
    "    def __init__(self, poison_ratio=0.2, target_class=3, optimization_steps=200, lr=0.01):\n",
    "        self.poison_ratio = poison_ratio\n",
    "        self.target_class = target_class\n",
    "        self.optimization_steps = optimization_steps\n",
    "        self.lr = lr\n",
    "    \n",
    "    def compute_influence_scores(self, model, dataset, num_samples=10000):\n",
    "        \"\"\"Compute influence-based scores for data selection\"\"\"\n",
    "        model.eval()\n",
    "        influence_scores = []\n",
    "        \n",
    "        # Sample a subset for efficiency\n",
    "        indices = random.sample(range(len(dataset)), min(num_samples, len(dataset)))\n",
    "        \n",
    "        for idx in indices:\n",
    "            data, target = dataset[idx]\n",
    "            data = data.unsqueeze(0).to(device)\n",
    "            target = torch.tensor([target]).to(device)\n",
    "            \n",
    "            # Compute influence using gradient-based approximation\n",
    "            data.requires_grad_(True)\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Compute influence score based on gradient magnitude and prediction confidence\n",
    "            grad_magnitude = torch.norm(data.grad).item()\n",
    "            confidence = F.softmax(output, dim=1).max().item()\n",
    "            \n",
    "            # Influence score combines gradient magnitude and prediction confidence\n",
    "            influence_score = grad_magnitude * (1 - confidence)  # Higher for uncertain predictions\n",
    "            influence_scores.append((idx, influence_score))\n",
    "            \n",
    "            data.requires_grad_(False)\n",
    "        \n",
    "        # Sort by influence score (highest first)\n",
    "        influence_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        return influence_scores\n",
    "    \n",
    "    def poison_dataset(self, dataset, model):\n",
    "        \"\"\"Poison dataset using witches' brew optimization\"\"\"\n",
    "        poisoned_dataset = deepcopy(dataset)\n",
    "        total_samples = len(poisoned_dataset)\n",
    "        num_poisoned = int(total_samples * self.poison_ratio)\n",
    "        \n",
    "        print(f\"Computing influence-based scores...\")\n",
    "        influence_scores = self.compute_influence_scores(model, dataset)\n",
    "        \n",
    "        # Select samples with highest influence scores\n",
    "        selected_indices = [idx for idx, _ in influence_scores[:num_poisoned]]\n",
    "        \n",
    "        print(f\"Selected {len(selected_indices)} samples based on influence scores\")\n",
    "        \n",
    "        # Poison selected samples \n",
    "        for idx in selected_indices:\n",
    "            data, _ = poisoned_dataset[idx]\n",
    "            data = data.unsqueeze(0).to(device)\n",
    "            \n",
    "            # Initialize perturbation\n",
    "            perturbation = torch.randn_like(data) * 0.1\n",
    "            perturbation.requires_grad_(True)\n",
    "            \n",
    "            optimizer = optim.Adam([perturbation], lr=self.lr)\n",
    "            \n",
    "            for step in range(self.optimization_steps):\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Add perturbation to data\n",
    "                perturbed_data = torch.clamp(data + perturbation, 0, 1)\n",
    "                \n",
    "                # Compute multiple objectives\n",
    "                output = model(perturbed_data)\n",
    "                target = torch.tensor([self.target_class]).to(device)\n",
    "                \n",
    "                # Primary objective: minimize loss for target class (make model confident in wrong prediction)\n",
    "                target_loss = F.cross_entropy(output, target)\n",
    "                \n",
    "                # Secondary objective: minimize confidence in original class\n",
    "                original_target = torch.tensor([poisoned_dataset.targets[idx]]).to(device)\n",
    "                original_loss = F.cross_entropy(output, original_target)\n",
    "                \n",
    "                # Combined objective\n",
    "                total_loss = target_loss + 0.5 * original_loss\n",
    "                \n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Project perturbation to valid range\n",
    "                #with torch.no_grad():\n",
    "                #    perturbation.clamp_(-0.5, 0.5)  # Allow larger perturbations\n",
    "            \n",
    "            # Apply optimized perturbation\n",
    "            final_perturbed_data = torch.clamp(data + perturbation, 0, 1)\n",
    "            poisoned_dataset.data[idx] = (final_perturbed_data.squeeze() * 255).to(torch.uint8)\n",
    "            poisoned_dataset.targets[idx] = self.target_class\n",
    "        \n",
    "        print(f\"Poisoned {len(selected_indices)} samples with witches' brew poisoning\")\n",
    "        return poisoned_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "478d260d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating poisoned datasets...\n"
     ]
    }
   ],
   "source": [
    "# Create poisoned datasets\n",
    "print(\"\\nCreating poisoned datasets...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bdc9fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poisoned 12000 samples with random label flipping attack\n"
     ]
    }
   ],
   "source": [
    "# Random label flipping\n",
    "label_flipping_attack = LabelFlippingAttack(poison_ratio=0.2, target_class=3)\n",
    "poisoned_train_dataset = label_flipping_attack.poison_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7d9a27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing gradient-based importance scores...\n",
      "Selected 10000 samples based on gradient importance\n",
      "Poisoned 10000 samples with optimized label flipping attack\n"
     ]
    }
   ],
   "source": [
    "# Gradient-optimized label flipping\n",
    "gradient_flipping_attack = OptimizedLabelFlippingAttack(poison_ratio=0.2, target_class=3, selection_method='gradient')\n",
    "gradient_poisoned_dataset = gradient_flipping_attack.poison_dataset(train_dataset, baseline_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841ea16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing gradient-based importance scores...\n",
      "Selected 10000 samples based on gradient importance\n"
     ]
    }
   ],
   "source": [
    "# Backgradient poisoning\n",
    "backgradient_attack = BackgradientPoisoningAttack(poison_ratio=0.2, target_class=3)\n",
    "backgradient_poisoned_dataset = backgradient_attack.poison_dataset(train_dataset, baseline_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6da019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models on poisoned data...\n"
     ]
    }
   ],
   "source": [
    "# Train models on poisoned data\n",
    "print(\"\\nTraining models on poisoned data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369adaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.6231\n",
      "Epoch [2/10], Loss: 0.5088\n",
      "Epoch [3/10], Loss: 0.4833\n",
      "Epoch [4/10], Loss: 0.4656\n",
      "Epoch [5/10], Loss: 0.4463\n",
      "Epoch [6/10], Loss: 0.4225\n",
      "Epoch [7/10], Loss: 0.3944\n",
      "Epoch [8/10], Loss: 0.3575\n",
      "Epoch [9/10], Loss: 0.3180\n",
      "Epoch [10/10], Loss: 0.2774\n",
      "Test Accuracy: 90.03%\n"
     ]
    }
   ],
   "source": [
    "# Train on random label flipped data\n",
    "poisoned_train_loader = DataLoader(poisoned_train_dataset, batch_size=64, shuffle=True)\n",
    "label_flipped_model = SimpleCNN().to(device)\n",
    "train_model(label_flipped_model, poisoned_train_loader, epochs=10, device=device)\n",
    "label_flipped_acc, label_flipped_preds, label_flip_targets = evaluate_model(label_flipped_model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0020d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15112067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.5785\n",
      "Epoch [2/10], Loss: 0.4625\n",
      "Epoch [3/10], Loss: 0.4391\n",
      "Epoch [4/10], Loss: 0.4207\n",
      "Epoch [5/10], Loss: 0.4001\n",
      "Epoch [6/10], Loss: 0.3760\n",
      "Epoch [7/10], Loss: 0.3461\n",
      "Epoch [8/10], Loss: 0.3122\n",
      "Epoch [9/10], Loss: 0.2737\n",
      "Epoch [10/10], Loss: 0.2363\n",
      "Test Accuracy: 91.79%\n"
     ]
    }
   ],
   "source": [
    "# Train on gradient-optimized label flipped data\n",
    "gradient_poisoned_loader = DataLoader(gradient_poisoned_dataset, batch_size=64, shuffle=True)\n",
    "gradient_flipped_model = SimpleCNN().to(device)\n",
    "train_model(gradient_flipped_model, gradient_poisoned_loader, epochs=10, device=device)\n",
    "gradient_flipped_acc, gradient_flipped_preds, gradient_flip_targets = evaluate_model(gradient_flipped_model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38b02edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "456f870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_gradient = confusion_matrix(gradient_flip_targets, gradient_flipped_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65515177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 928,    0,    0,   48,    0,    0,    2,    1,    0,    1],\n",
       "       [   0, 1090,    0,   44,    0,    0,    1,    0,    0,    0],\n",
       "       [   1,    0,  899,  125,    1,    0,    1,    4,    0,    1],\n",
       "       [   0,    1,    2, 1002,    0,    4,    0,    1,    0,    0],\n",
       "       [   0,    0,    0,  104,  870,    0,    1,    0,    4,    3],\n",
       "       [   1,    0,    0,  112,    0,  775,    2,    0,    2,    0],\n",
       "       [   5,    0,    0,   67,    0,    4,  882,    0,    0,    0],\n",
       "       [   0,    2,    6,   60,    0,    0,    0,  955,    3,    2],\n",
       "       [   0,    0,    0,  107,    0,    0,    0,    0,  866,    1],\n",
       "       [   0,    0,    0,   87,    3,    4,    0,    2,    1,  912]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix_gradient"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
